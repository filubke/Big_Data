{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNibkIYk5qW4mwEmivMyjoy"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Iniciando o Pyspark no google Colab\n",
        "\n"
      ],
      "metadata": {
        "id": "rJOiUNWeEWVH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chjvhbC0Cyk_",
        "outputId": "dd1cab78-a4cf-4e00-bad6-e96cf83daceb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.12/dist-packages (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "appName = \"PySpark Trabalho de Big Data\"\n",
        "master = \"local\"\n",
        "\n",
        "spark = SparkSession.builder.appName(appName).master(master).getOrCreate()"
      ],
      "metadata": {
        "id": "yEETz_VTEPFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baixar, extrair e deletar o .zip do kagle"
      ],
      "metadata": {
        "id": "9vfm72l0Fb2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip -O imdb-reviews-pt-br.zip\n",
        "!unzip imdb-reviews-pt-br.zip\n",
        "!rm imdb-reviews-pt-br.zip"
      ],
      "metadata": {
        "id": "hqIr78T5FPvF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e74dc6e9-355c-4f14-dc23-c6280d80e8a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-25 01:42:38--  https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49549692 (47M) [application/zip]\n",
            "Saving to: ‘imdb-reviews-pt-br.zip’\n",
            "\n",
            "imdb-reviews-pt-br. 100%[===================>]  47.25M  52.6MB/s    in 0.9s    \n",
            "\n",
            "2025-08-25 01:42:45 (52.6 MB/s) - ‘imdb-reviews-pt-br.zip’ saved [49549692/49549692]\n",
            "\n",
            "Archive:  imdb-reviews-pt-br.zip\n",
            "  inflating: imdb-reviews-pt-br.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subir data frame na versão do spark (que esta no colab)"
      ],
      "metadata": {
        "id": "dwgTAC0fFn_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_df = spark.read.csv('imdb-reviews-pt-br.csv',\n",
        "                         header=True,\n",
        "                         quote=\"\\\"\",\n",
        "                         escape=\"\\\"\",\n",
        "                         encoding=\"UTF-8\")"
      ],
      "metadata": {
        "id": "xEnF5XNfFMAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meu_ru = 5003159"
      ],
      "metadata": {
        "id": "2nZIadk4Nalb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_lPWoFqDWTt"
      },
      "source": [
        "# Questão 1\n",
        "\n",
        "Nesta questão, você irá calcular a soma dos IDs para entradas onde o sentimento ('sentiment') é 'neg'.\n",
        "\n",
        "### Objetivo:\n",
        "- Usar a coluna 'sentiment' como chave e somar os valores da coluna 'id'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxJrUjbmDWTu"
      },
      "source": [
        "## Criar funções de MAP:\n",
        "- Criar função para mapear o \"sentiment\" como chave e o \"id\" como valor do tipo inteiro\n",
        "\n",
        "A função map irá transformar cada linha do dataframe em uma **tupla** (chave-valor), onde:\n",
        "- Chave: coluna 'sentiment'\n",
        "- Valor: coluna 'id' convertida para inteiro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNhgTo6bDWTu"
      },
      "outputs": [],
      "source": [
        "def map1(x):\n",
        "  # Esta função deve retornar uma tupla (sentiment, id).\n",
        "  # o argumento de entrada x é uma linha do dataframe imdb_df\n",
        "  # com os dados em um formato semelhante à um dicionário\n",
        "  return (x['sentiment'], int(x['id']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dtZa6R0DWTu"
      },
      "source": [
        "## Cria funções de REDUCE:\n",
        "\n",
        "- Criar função de reduce para somar os IDs por \"sentiment\".\n",
        "\n",
        "A função reduce irá somar os valores dos IDs agrupados por chave ('sentiment')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AijlRaxcDWTu"
      },
      "outputs": [],
      "source": [
        "def reduceByKey1(x,y):\n",
        "  # Coloque aqui o seu código para retornar o resultado necessário.\n",
        "  # x e y são valores que serão somados, pois o reduceByKey receberá\n",
        "  # apenas o segundo elemento da tupla vinda da saída da função map.\n",
        "  return x + y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_47ohjHDWTu"
      },
      "source": [
        "## Aplicação do map/reduce e visualização do resultado\n",
        "\n",
        "Aqui, você aplicará as funções de map e reduce ao dataframe Spark para calcular os resultados. Não se esqueça de usar o método `.collect()` para visualizar os resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9dxfqohDWTu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edd08f6b-b86d-4f32-8e39-438795741417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RU: 5003159 | Resultado da soma dos IDs por sentimento: [('neg', 459568555)]\n"
          ]
        }
      ],
      "source": [
        "# Linha de código para aplicar o map/reduce no seu dataframe spark\n",
        "resultado = imdb_df.filter(imdb_df.sentiment == 'neg').rdd.map(map1).reduceByKey(reduceByKey1).collect()\n",
        "# Coloque aqui o código para imprimir o resultado. Não esqueça seu RU:\n",
        "print(f\"RU: {meu_ru} | Resultado da soma dos IDs por sentimento: {resultado}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1BB4nJ8jF4PP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "appName = \"PySpark Trabalho de Big Data\"\n",
        "master = \"local\"\n",
        "spark = SparkSession.builder.appName(appName).master(master).getOrCreate()!wget https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip -O imdb-reviews-pt-br.zip\n",
        "\n",
        "!unzip imdb-reviews-pt-br.zip\n",
        "!rm imdb-reviews-pt-br.zip\n",
        "\n",
        "imdb_df = spark.read.csv('imdb-reviews-pt-br.csv',\n",
        "                         header=True,\n",
        "                         quote=\"\\\"\",\n",
        "                         escape=\"\\\"\",\n",
        "                         encoding=\"UTF-8\")\n",
        "\n",
        "meu_ru = 5003159\n",
        "\n",
        "def map1(x):\n",
        "  # Esta função deve retornar uma tupla (sentiment, id).\n",
        "  # o argumento de entrada x é uma linha do dataframe imdb_df\n",
        "  # com os dados em um formato semelhante à um dicionário\n",
        "  return (x['sentiment'], int(x['id']))\n",
        "\n",
        "def reduceByKey1(x,y):\n",
        "  # Coloque aqui o seu código para retornar o resultado necessário.\n",
        "  # x e y são valores que serão somados, pois o reduceByKey receberá\n",
        "  # apenas o segundo elemento da tupla vinda da saída da função map.\n",
        "  return x + y\n",
        "\n",
        "# Linha de código para aplicar o map/reduce no seu dataframe spark\n",
        "resultado = imdb_df.filter(imdb_df.sentiment == 'neg').rdd.map(map1).reduceByKey(reduceByKey1).collect()\n",
        "# Coloque aqui o código para imprimir o resultado. Não esqueça seu RU:\n",
        "print(f\"RU: {meu_ru} | Resultado da soma dos IDs por sentimento: {resultado}\")"
      ],
      "metadata": {
        "id": "sisCmFdF5lkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-L6EJADDWTv"
      },
      "source": [
        "# Questão 2:\n",
        "\n",
        "Nesta questão, você irá calcular a diferença no número total de palavras entre textos negativos em português e inglês.\n",
        "\n",
        "### Objetivo:\n",
        "- Contar as palavras em cada idioma (colunas 'text_pt' e 'text_en') para entradas onde o sentimento ('sentiment') é 'neg'.\n",
        "- Subtrair o total de palavras em inglês do total em português."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKX6NxL9DWTv"
      },
      "source": [
        "## Criar funções de MAP:\n",
        "- Criar função para mapear o \"sentiment\" como chave de uma tupla principal e como valor uma outra tupla com a soma das palavras de cada idioma como valor.\n",
        "\n",
        "A função map irá transformar cada linha do dataframe em uma tupla (chave-valor), onde:\n",
        "- Chave: coluna 'sentiment'\n",
        "- Valor: Nova tupla com:\n",
        "  - Elemento 0: soma das palavras da coluna 'text_en'\n",
        "  - Elemento 1: soma das palavras da coluna 'text_pt'\n",
        "\n",
        "OU\n",
        "- Chave: coluna 'sentiment'\n",
        "- Valor: (soma das palavras da coluna 'text_pt') - (soma das palavras da coluna 'text_en')\n",
        "  \n",
        "\n",
        "Para contar as palavras deve-se primeiro separar os textos em uma lista de palavras para então descobrir o tamanho desta lista.\n",
        "Dicas:\n",
        "\n",
        "1. Use o método .split() e não .split(\" \") de string para separar as palavras em uma lista ou use a função split(coluna de texto, regex) do pyspark com o regex igual à \"[ ]+\" ou \"\\s+\"\n",
        "2. Use len() para descobrir o tamanho da lista de palavras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xM2osBvSDWTv"
      },
      "outputs": [],
      "source": [
        "def map2(x):\n",
        "  # Coloque aqui o seu código para retornar a tupla necessária.\n",
        "  # Sugerimos o retorno em uma tupla com este formato:\n",
        "  #   (sentiment, (quantidade_de_palavras_em_en, quantidade_de_palavras_em_pt))\n",
        "  #   OU\n",
        "  #   (sentiment, quantidade_de_caracteres_em_pt-quantidade_de_caracteres_em_en)\n",
        "\n",
        "  # Obtém o número de palavras para 'text_en' e 'text_pt'\n",
        "  palavras_en = len(x['text_en'].split()) if x['text_en'] else 0\n",
        "  palavras_pt = len(x['text_pt'].split()) if x['text_pt'] else 0\n",
        "  # Retorna a tupla no formato (chave, valor)\n",
        "  return (x['sentiment'], (palavras_en, palavras_pt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGlf8OPIDWTv"
      },
      "source": [
        "## Cria funções de REDUCE:\n",
        "\n",
        "- Criar função de reduce para somar o numero de palavras de cada texto português e inglês por \"sentiment\" (dependerá de como você optou por fazer sua função map2).\n",
        "\n",
        "A função reduce irá somar os valores das quantidades de palavras agrupados por chave ('sentiment')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1C8zvj8DWTv"
      },
      "outputs": [],
      "source": [
        "def reduceByKey2(x,y):\n",
        "  # Coloque aqui o seu código para retornar o resultado necessário.\n",
        "  # x e y são valores que podem ser ou a tupla vinda da saída da função map\n",
        "  # contendo quantidade de palavras em inglês e português, ou a diferença, a\n",
        "  # depender da sua implementação da função map2.\n",
        "  return (x[0] + y[0], x[1] + y[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7wr__maDWTv"
      },
      "source": [
        "## Aplicação do map/reduce e visualização do resultado\n",
        "\n",
        "1. Aplicar o map/reduce no seu dataframe spark e realizar o collect() ao final\n",
        "2. Selecionar os dados referentes aos textos negativos para realizar a subtração.\n",
        "3. Realizar a subtração das contagens de palavras dos textos negativos para obter o resultado final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5u1Irv7BDWTv"
      },
      "outputs": [],
      "source": [
        "# Linha de código para aplicar o map/reduce no seu dataframe spark\n",
        "resultado_negativos = imdb_df.filter(imdb_df.sentiment == 'neg').rdd.map(map2).reduceByKey(reduceByKey2).collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Coloque aqui suas linhas de código final para imprimir o resultado.\n",
        "if resultado_negativos:\n",
        "  total_palavras_en = resultado_negativos[0][1][0]\n",
        "  total_palavras_pt = resultado_negativos[0][1][1]\n",
        "  diferenca = total_palavras_pt - total_palavras_en\n",
        "  print(f\"RU: {meu_ru}\")\n",
        "  print(f\"Total de palavras em Português nos textos negativos: {total_palavras_pt}\")\n",
        "  print(f\"Total de palavras em Inglês nos textos negativos: {total_palavras_en}\")\n",
        "  print(f\"A diferença de palavras (Português - Inglês) é: {diferenca}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdulPQVCH-Xd",
        "outputId": "8f3f2b75-300b-4210-ab3c-ce648256f92a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RU: 5003159\n",
            "Total de palavras em Português nos textos negativos: 5455273\n",
            "Total de palavras em Inglês nos textos negativos: 5400324\n",
            "A diferença de palavras (Português - Inglês) é: 54949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meu_ru = 5003159\n",
        "\n",
        "def map2(x):\n",
        "  # Coloque aqui o seu código para retornar a tupla necessária.\n",
        "  # Sugerimos o retorno em uma tupla com este formato:\n",
        "  #   (sentiment, (quantidade_de_palavras_em_en, quantidade_de_palavras_em_pt))\n",
        "  #   OU\n",
        "  #   (sentiment, quantidade_de_caracteres_em_pt-quantidade_de_caracteres_em_en)\n",
        "\n",
        "  # Obtém o número de palavras para 'text_en' e 'text_pt'\n",
        "  palavras_en = len(x['text_en'].split()) if x['text_en'] else 0\n",
        "  palavras_pt = len(x['text_pt'].split()) if x['text_pt'] else 0\n",
        "  # Retorna a tupla no formato (chave, valor)\n",
        "  return (x['sentiment'], (palavras_en, palavras_pt))\n",
        "\n",
        "def reduceByKey2(x,y):\n",
        "  # Coloque aqui o seu código para retornar o resultado necessário.\n",
        "  # x e y são valores que podem ser ou a tupla vinda da saída da função map\n",
        "  # contendo quantidade de palavras em inglês e português, ou a diferença, a\n",
        "  # depender da sua implementação da função map2.\n",
        "  return (x[0] + y[0], x[1] + y[1])\n",
        "\n",
        "# Linha de código para aplicar o map/reduce no seu dataframe spark\n",
        "resultado_negativos = imdb_df.filter(imdb_df.sentiment == 'neg').rdd.map(map2).reduceByKey(reduceByKey2).collect()\n",
        "\n",
        "# Coloque aqui suas linhas de código final para imprimir o resultado.\n",
        "if resultado_negativos:\n",
        "  total_palavras_en = resultado_negativos[0][1][0]\n",
        "  total_palavras_pt = resultado_negativos[0][1][1]\n",
        "  diferenca = total_palavras_pt - total_palavras_en\n",
        "  print(f\"RU: {meu_ru}\")\n",
        "  print(f\"Total de palavras em Português nos textos negativos: {total_palavras_pt}\")\n",
        "  print(f\"Total de palavras em Inglês nos textos negativos: {total_palavras_en}\")\n",
        "  print(f\"A diferença de palavras (Português - Inglês) é: {diferenca}\")"
      ],
      "metadata": {
        "id": "WM6xbiBZINd8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}