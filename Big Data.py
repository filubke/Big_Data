# -*- coding: utf-8 -*-
"""Trabalho Big data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/163BGYF-bKD_xNkV_U2b1vFPQHrki9D8C

# Iniciando o Pyspark no google Colab
"""

!pip install pyspark
!pip install findspark

from pyspark.sql import SparkSession

appName = "PySpark Trabalho de Big Data"
master = "local"

spark = SparkSession.builder.appName(appName).master(master).getOrCreate()

"""Baixar, extrair e deletar o .zip do kagle"""

!wget https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip -O imdb-reviews-pt-br.zip
!unzip imdb-reviews-pt-br.zip
!rm imdb-reviews-pt-br.zip

"""Subir data frame na versão do spark (que esta no colab)"""

imdb_df = spark.read.csv('imdb-reviews-pt-br.csv',
                         header=True,
                         quote="\"",
                         escape="\"",
                         encoding="UTF-8")

meu_ru = 5003159

"""# Questão 1

Nesta questão, você irá calcular a soma dos IDs para entradas onde o sentimento ('sentiment') é 'neg'.

### Objetivo:
- Usar a coluna 'sentiment' como chave e somar os valores da coluna 'id'.

## Criar funções de MAP:
- Criar função para mapear o "sentiment" como chave e o "id" como valor do tipo inteiro

A função map irá transformar cada linha do dataframe em uma **tupla** (chave-valor), onde:
- Chave: coluna 'sentiment'
- Valor: coluna 'id' convertida para inteiro.
"""

def map1(x):
  # Esta função deve retornar uma tupla (sentiment, id).
  # o argumento de entrada x é uma linha do dataframe imdb_df
  # com os dados em um formato semelhante à um dicionário
  return (x['sentiment'], int(x['id']))

"""## Cria funções de REDUCE:

- Criar função de reduce para somar os IDs por "sentiment".

A função reduce irá somar os valores dos IDs agrupados por chave ('sentiment').
"""

def reduceByKey1(x,y):
  # Coloque aqui o seu código para retornar o resultado necessário.
  # x e y são valores que serão somados, pois o reduceByKey receberá
  # apenas o segundo elemento da tupla vinda da saída da função map.
  return x + y

"""## Aplicação do map/reduce e visualização do resultado

Aqui, você aplicará as funções de map e reduce ao dataframe Spark para calcular os resultados. Não se esqueça de usar o método `.collect()` para visualizar os resultados.
"""

# Linha de código para aplicar o map/reduce no seu dataframe spark
resultado = imdb_df.filter(imdb_df.sentiment == 'neg').rdd.map(map1).reduceByKey(reduceByKey1).collect()
# Coloque aqui o código para imprimir o resultado. Não esqueça seu RU:
print(f"RU: {meu_ru} | Resultado da soma dos IDs por sentimento: {resultado}")

!pip install pyspark
!pip install findspark

from pyspark.sql import SparkSession
appName = "PySpark Trabalho de Big Data"
master = "local"
spark = SparkSession.builder.appName(appName).master(master).getOrCreate()!wget https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip -O imdb-reviews-pt-br.zip

!unzip imdb-reviews-pt-br.zip
!rm imdb-reviews-pt-br.zip

imdb_df = spark.read.csv('imdb-reviews-pt-br.csv',
                         header=True,
                         quote="\"",
                         escape="\"",
                         encoding="UTF-8")

meu_ru = 5003159

def map1(x):
  # Esta função deve retornar uma tupla (sentiment, id).
  # o argumento de entrada x é uma linha do dataframe imdb_df
  # com os dados em um formato semelhante à um dicionário
  return (x['sentiment'], int(x['id']))

def reduceByKey1(x,y):
  # Coloque aqui o seu código para retornar o resultado necessário.
  # x e y são valores que serão somados, pois o reduceByKey receberá
  # apenas o segundo elemento da tupla vinda da saída da função map.
  return x + y

# Linha de código para aplicar o map/reduce no seu dataframe spark
resultado = imdb_df.filter(imdb_df.sentiment == 'neg').rdd.map(map1).reduceByKey(reduceByKey1).collect()
# Coloque aqui o código para imprimir o resultado. Não esqueça seu RU:
print(f"RU: {meu_ru} | Resultado da soma dos IDs por sentimento: {resultado}")

"""# Questão 2:

Nesta questão, você irá calcular a diferença no número total de palavras entre textos negativos em português e inglês.

### Objetivo:
- Contar as palavras em cada idioma (colunas 'text_pt' e 'text_en') para entradas onde o sentimento ('sentiment') é 'neg'.
- Subtrair o total de palavras em inglês do total em português.

## Criar funções de MAP:
- Criar função para mapear o "sentiment" como chave de uma tupla principal e como valor uma outra tupla com a soma das palavras de cada idioma como valor.

A função map irá transformar cada linha do dataframe em uma tupla (chave-valor), onde:
- Chave: coluna 'sentiment'
- Valor: Nova tupla com:
  - Elemento 0: soma das palavras da coluna 'text_en'
  - Elemento 1: soma das palavras da coluna 'text_pt'

OU
- Chave: coluna 'sentiment'
- Valor: (soma das palavras da coluna 'text_pt') - (soma das palavras da coluna 'text_en')
  

Para contar as palavras deve-se primeiro separar os textos em uma lista de palavras para então descobrir o tamanho desta lista.
Dicas:

1. Use o método .split() e não .split(" ") de string para separar as palavras em uma lista ou use a função split(coluna de texto, regex) do pyspark com o regex igual à "[ ]+" ou "\s+"
2. Use len() para descobrir o tamanho da lista de palavras.
"""

def map2(x):
  # Coloque aqui o seu código para retornar a tupla necessária.
  # Sugerimos o retorno em uma tupla com este formato:
  #   (sentiment, (quantidade_de_palavras_em_en, quantidade_de_palavras_em_pt))
  #   OU
  #   (sentiment, quantidade_de_caracteres_em_pt-quantidade_de_caracteres_em_en)

  # Obtém o número de palavras para 'text_en' e 'text_pt'
  palavras_en = len(x['text_en'].split()) if x['text_en'] else 0
  palavras_pt = len(x['text_pt'].split()) if x['text_pt'] else 0
  # Retorna a tupla no formato (chave, valor)
  return (x['sentiment'], (palavras_en, palavras_pt))

"""## Cria funções de REDUCE:

- Criar função de reduce para somar o numero de palavras de cada texto português e inglês por "sentiment" (dependerá de como você optou por fazer sua função map2).

A função reduce irá somar os valores das quantidades de palavras agrupados por chave ('sentiment').
"""

def reduceByKey2(x,y):
  # Coloque aqui o seu código para retornar o resultado necessário.
  # x e y são valores que podem ser ou a tupla vinda da saída da função map
  # contendo quantidade de palavras em inglês e português, ou a diferença, a
  # depender da sua implementação da função map2.
  return (x[0] + y[0], x[1] + y[1])

"""## Aplicação do map/reduce e visualização do resultado

1. Aplicar o map/reduce no seu dataframe spark e realizar o collect() ao final
2. Selecionar os dados referentes aos textos negativos para realizar a subtração.
3. Realizar a subtração das contagens de palavras dos textos negativos para obter o resultado final
"""

# Linha de código para aplicar o map/reduce no seu dataframe spark
resultado_negativos = imdb_df.filter(imdb_df.sentiment == 'neg').rdd.map(map2).reduceByKey(reduceByKey2).collect()

# Coloque aqui suas linhas de código final para imprimir o resultado.
if resultado_negativos:
  total_palavras_en = resultado_negativos[0][1][0]
  total_palavras_pt = resultado_negativos[0][1][1]
  diferenca = total_palavras_pt - total_palavras_en
  print(f"RU: {meu_ru}")
  print(f"Total de palavras em Português nos textos negativos: {total_palavras_pt}")
  print(f"Total de palavras em Inglês nos textos negativos: {total_palavras_en}")
  print(f"A diferença de palavras (Português - Inglês) é: {diferenca}")

meu_ru = 5003159

def map2(x):
  # Coloque aqui o seu código para retornar a tupla necessária.
  # Sugerimos o retorno em uma tupla com este formato:
  #   (sentiment, (quantidade_de_palavras_em_en, quantidade_de_palavras_em_pt))
  #   OU
  #   (sentiment, quantidade_de_caracteres_em_pt-quantidade_de_caracteres_em_en)

  # Obtém o número de palavras para 'text_en' e 'text_pt'
  palavras_en = len(x['text_en'].split()) if x['text_en'] else 0
  palavras_pt = len(x['text_pt'].split()) if x['text_pt'] else 0
  # Retorna a tupla no formato (chave, valor)
  return (x['sentiment'], (palavras_en, palavras_pt))

def reduceByKey2(x,y):
  # Coloque aqui o seu código para retornar o resultado necessário.
  # x e y são valores que podem ser ou a tupla vinda da saída da função map
  # contendo quantidade de palavras em inglês e português, ou a diferença, a
  # depender da sua implementação da função map2.
  return (x[0] + y[0], x[1] + y[1])

# Linha de código para aplicar o map/reduce no seu dataframe spark
resultado_negativos = imdb_df.filter(imdb_df.sentiment == 'neg').rdd.map(map2).reduceByKey(reduceByKey2).collect()

# Coloque aqui suas linhas de código final para imprimir o resultado.
if resultado_negativos:
  total_palavras_en = resultado_negativos[0][1][0]
  total_palavras_pt = resultado_negativos[0][1][1]
  diferenca = total_palavras_pt - total_palavras_en
  print(f"RU: {meu_ru}")
  print(f"Total de palavras em Português nos textos negativos: {total_palavras_pt}")
  print(f"Total de palavras em Inglês nos textos negativos: {total_palavras_en}")
  print(f"A diferença de palavras (Português - Inglês) é: {diferenca}")